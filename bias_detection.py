# # -*- coding: utf-8 -*-
# """bias_detection.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1W4ZbknNPmqC1tw3mNOhgn_pi4psqjo0U
# """

# !pip install torchinfo
# from google.colab import drive
# drive.mount('/content/drive')

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import os
from torchvision import datasets, transforms
from torch.utils.data import random_split, ConcatDataset

import matplotlib.pyplot as plt
from torchinfo import summary

from sklearn.metrics import f1_score, confusion_matrix

from tqdm import tqdm
import pandas as pd
import sys
import torch.nn.functional as F


# Setting the working directory:

# if not('Nilly' in os.getcwd() or '472data' in os.getcwd()):
#   # os.chdir('drive/MyDrive/Nilly/')
#   os.chdir('drive/MyDrive/472data/')

# specifying to use GPU of computer first, if not, THEN use the CPU
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(device)

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import os
from torchvision import datasets, transforms
from torch.utils.data import random_split


class ConvNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=5):
        super(ConvNet, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding='same', bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.dropout1 = nn.Dropout(p=0.3)

        self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, stride=1, padding='same', bias=False)
        self.bn2 = nn.BatchNorm2d(64)
        self.dropout2 = nn.Dropout(p=0.3)

        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same', bias=False)
        self.bn3 = nn.BatchNorm2d(128)
        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 48x48 -> 24x24
        self.dropout3 = nn.Dropout(p=0.3)

        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='same', bias=False)
        self.bn4 = nn.BatchNorm2d(128)
        self.dropout4 = nn.Dropout(p=0.3)

        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='same', bias=False)
        self.bn5 = nn.BatchNorm2d(128)
        self.dropout5 = nn.Dropout(p=0.3)

        self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='same', bias=False)
        self.bn6 = nn.BatchNorm2d(128)
        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 24x24 -> 12x12
        self.dropout6 = nn.Dropout(p=0.3)

        self.conv7 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn7 = nn.BatchNorm2d(64)
        self.dropout7 = nn.Dropout(p=0.3)

        self.conv8 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=2, padding=0, bias=False)
        self.bn8 = nn.BatchNorm2d(16)
        self.max_pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 12x12 -> 6x6
        self.dropout8 = nn.Dropout(p=0.3)

        self.fc1 = nn.Linear(in_features=6*6*16, out_features=256)
        self.fc2 = nn.Linear(in_features=256, out_features=32)
        self.fc3 = nn.Linear(32, out_channels)


    def forward(self, x):

        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout1(x) # <- block 1
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout2(x) # <- block 2
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.max_pool1(x)
        x = self.dropout3(x) # <- block 3

        x = self.conv4(x)
        x = self.bn4(x)
        x = F.relu(x)
        x = self.dropout4(x) # <- block 4
        x = self.conv5(x)
        x = self.bn5(x)
        x = F.relu(x)
        x = self.dropout5(x) # <- block 5
        x = self.conv6(x)
        x = self.bn6(x)
        x = F.relu(x)
        x = self.max_pool2(x)
        x = self.dropout6(x) # <- block 6

        x = self.conv7(x)
        x = self.bn7(x)
        x = F.relu(x)
        x = self.dropout7(x) # <- block 7
        x = self.conv8(x)
        x = self.bn8(x)
        x = F.relu(x)
        x = self.max_pool3(x)
        x = self.dropout8(x) # <- block 8

        x = torch.flatten(x, start_dim=1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        x = F.softmax(x, dim=1)

        return x

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import os
from torchvision import datasets, transforms
from torch.utils.data import random_split

import matplotlib.pyplot as plt
from torchinfo import summary

from sklearn.metrics import f1_score, confusion_matrix

from tqdm import tqdm
import pandas as pd
import sys
import torch.nn.functional as F

from skimage import io, transform

from pathlib import Path
import pathlib
from PIL import Image

# THIS WAS MY WORKING DIRECTORY, SET YOUR OWN FOR EACH DEVICE
if not('Nilly' in os.getcwd() or '472data' in os.getcwd()):
  # os.chdir('drive/MyDrive/Nilly/')
  os.chdir('drive/MyDrive/472data/')

EPOCHS = 60
lr = 0.001
b_s = 20

# EVERY TIME YOU RUN A DIFFERENT MODEL, CHAN
try_num = 1

dir = 'COMP472-AK_15-main/'
save_dir = 'model_' + str(try_num) + '/'

if not os.path.isdir(save_dir):
  os.mkdir(save_dir)

transform = transforms.Compose([
  transforms.Resize([224, 224]),
  # transforms.CenterCrop(224),
  transforms.Grayscale(num_output_channels=1),
  transforms.ToTensor(),
  # transforms.RandomRotation(30),
  # transforms.RandomHorizontalFlip(),
  transforms.Normalize(mean=[0.5], std=[0.2]), # Use single-channel values for grayscale
  ])
test_transform = transforms.Compose([
  transforms.Resize([224, 224]),
  transforms.Grayscale(num_output_channels=1),
  # transforms.CenterCrop(224),
  transforms.ToTensor(),
  transforms.Normalize(mean=[0.5], std=[0.2]), # Use single-channel values for grayscale
  ])

train_dataset2 = datasets.ImageFolder(root=dir + 'dataset/train', transform=transform)
test_dataset2 = datasets.ImageFolder(root=dir + 'dataset/test', transform=transform)

test_dataloader2 = DataLoader(test_dataset2, batch_size=b_s, shuffle=True)

# label_dict_train = {}
# for image in train_dataset2.imgs:
#   path, label = image
#   label_dict_train[path.split('/')[-1]] = label


# label_dict_test = {}
# for image in test_dataset2.imgs:
#   path, label = image
#   label_dict_test[path.split('/')[-1]] = label

class CostumDataset(Dataset):

    # 2. Initialize with a targ_dir and transform (optional) parameter
    def __init__(self, targ_dir: str, label_dict, transform=None) -> None:

        # 3. Create class attributes
        # Get all image paths
        self.paths = list(pathlib.Path(targ_dir).glob("*.png")) # note: you'd have to update this if you've got .png's or .jpeg's
        # Setup transforms
        self.transform = transform
        self.label_dict = label_dict

    def find_classes(self, path):
        name = str(path).split('/')[-1].replace('_', '').split(' ')[0]
        if '.png' not in name:
          name += '.png'
        return self.label_dict[name]

    # 4. Make function to load images
    def load_image(self, index: int):
        "Opens an image via a path and returns it."
        image_path = self.paths[index]
        return Image.open(image_path)

    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)
    def __len__(self) -> int:
        "Returns the total number of samples."
        return len(self.paths)

    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)
    def __getitem__(self, index: int):
        "Returns one sample of data, data and label (X, y)."
        img = self.load_image(index)
        target  = self.find_classes(self.paths[index])

        # Transform if necessary
        if self.transform:
            return self.transform(img), target # return data, label (X, y)
        else:
            return img, target # return data, label (X, y)

# test_dataset_male_young = datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/male/young', transform=transform)
# test_dataset_male_senior = CostumDataset(dir + 'dataset_bias/dataset_biased/test/male/senior', label_dict_test, transform=transform)
# test_dataset_male_middle = CostumDataset(dir + 'dataset_bias/dataset_biased/test/male/middle-age', label_dict_test, transform=transform)

# test_dataset_female_young = CostumDataset(dir + 'dataset_bias/dataset_biased/test/female/young', label_dict_test, transform=transform)
# test_dataset_female_senior = CostumDataset(dir + 'dataset_bias/dataset_biased/test/female/senior', label_dict_test, transform=transform)
# test_dataset_female_middle = CostumDataset(dir + 'dataset_bias/dataset_biased/test/female/middle-age', label_dict_test, transform=transform)


test_dataset_male = ConcatDataset([datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/male/young', transform=transform),
                                                    datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/male/senior', transform=transform),
                                                    datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/male/middle-age', transform=transform)])
test_dataset_female = ConcatDataset([datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/female/young', transform=transform),
                                                    datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/female/senior', transform=transform),
                                                    datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/female/middle-age', transform=transform)])


test_dataset_young = ConcatDataset([datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/male/young', transform=transform),
                                                     datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/female/young', transform=transform)])
test_dataset_senior = ConcatDataset([datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/male/senior', transform=transform),
                                                     datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/female/senior', transform=transform)])
test_dataset_middle = ConcatDataset([datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/male/middle-age', transform=transform),
                                                     datasets.ImageFolder(root=dir + 'dataset_bias/dataset_biased/test/female/middle-age', transform=transform)])

# data loader batches the data and readies it for the model
test_dataloader_male = DataLoader(test_dataset_male, batch_size=b_s, shuffle=True)
test_dataloader_female = DataLoader(test_dataset_female, batch_size=b_s, shuffle=True)

test_dataloader_young = DataLoader(test_dataset_young, batch_size=b_s, shuffle=True)
test_dataloader_senior = DataLoader(test_dataset_senior, batch_size=b_s, shuffle=True)
test_dataloader_middle = DataLoader(test_dataset_middle, batch_size=b_s, shuffle=True)

model = ConvNet()
summary(model, input_size=(b_s, 1, 224, 224))

model.load_state_dict(torch.load(save_dir + 'convnet3_deeper.pt'))

from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix


def evaluation(model, test_dataloader):
  # model.load_state_dict(torch.load(save_dir + 'model.pt', map_location=device))
  model.to(device)
  model.eval()

  preds = []
  labels = []
  for i, batch in enumerate(tqdm(test_dataloader)):
      with torch.no_grad():

        img_batch, label_batch = batch
        img_batch = img_batch.to(device)
        label_batch = label_batch.type(torch.LongTensor).to(device)

        # with torch.cuda.amp.autocast():
        output = model(img_batch)

        preds += torch.argmax(output, dim=1).detach().cpu().numpy().tolist()
        labels += label_batch.detach().cpu().numpy().tolist()

  labels = np.array(labels)
  preds = np.array(preds)

  # Calculate macro-averaged precision, recall, and F1-score
  precision = precision_score(labels, preds, average='macro')
  recall = recall_score(labels, preds, average='macro')
  f1 = f1_score(labels, preds, average='macro')

  # Calculate overall accuracy
  accuracy = (preds == labels).sum() / len(test_dataloader.dataset)

  # c = multilabel_confusion_matrix(labels, preds)

  # Print the results
  print("")
  print('Precision:', precision)
  print('Recall:', recall)
  print('F1-score:', f1)
  print('Overall Accuracy:', accuracy)

  return labels, preds

labels, preds = evaluation(model, test_dataloader2)

labels, preds = evaluation(model, test_dataloader_male)

labels, preds = evaluation(model, test_dataloader_female)

labels, preds = evaluation(model, test_dataloader_young)

labels, preds = evaluation(model, test_dataloader_senior)

_ = evaluation(model, test_dataloader_middle)

